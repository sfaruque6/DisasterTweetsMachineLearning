{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1yRtz0HAZ0jjFt3PXA9E_wSpaoxvuGAZr","timestamp":1702936980617},{"file_id":"1T71WFbWYa6SzUXxIm49bJxwLJQps4dYy","timestamp":1679159540487}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"pRJjnpyWtSDG","executionInfo":{"status":"ok","timestamp":1709766392834,"user_tz":300,"elapsed":9438,"user":{"displayName":"Sarim Faruque","userId":"18008571687384996970"}}},"outputs":[],"source":["# importing all the needed standard libraries\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import tensorflow as tf\n","import seaborn as sns\n","sns.set(style=\"darkgrid\")\n","import sklearn.metrics\n","from sklearn.linear_model import LogisticRegression\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["# mounting google drive to google colab\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-RmVZbYdxdGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# loading training and testing data\n","train_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/train.csv')\n","test_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/test.csv')\n","train_df.shape\n","# train_df[:50]"],"metadata":{"id":"NQYLTZ2ox6hA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting the training data into 90% for training and 10% for validation\n","training_data, validation_data = train_test_split(train_df, test_size=0.1, random_state=25)\n","print(training_data.shape,validation_data.shape)\n","print(training_data)"],"metadata":{"id":"0TdMQmPr07ei"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # find a baseline model\n","def disaster_baseline(text):\n","    if \"#\" in text or \"http\" in text:\n","        return 1\n","    else:\n","        return 0\n","\n","# for training data\n","train_preds = [disaster_baseline(x) for x in training_data['text']]\n","print('accuracy', sklearn.metrics.accuracy_score(training_data['target'],train_preds))\n","print('f1', sklearn.metrics.f1_score(training_data['target'],train_preds))\n","print(sklearn.metrics.classification_report(training_data['target'],train_preds))\n","\n","# for validation data\n","val_preds = [disaster_baseline(x) for x in validation_data['text']]\n","print('accuracy', sklearn.metrics.accuracy_score(validation_data['target'],val_preds))\n","print('f1', sklearn.metrics.f1_score(validation_data['target'],val_preds))\n","print(sklearn.metrics.classification_report(validation_data['target'],val_preds))"],"metadata":{"id":"qE31LgWT6CC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Output CSV\n","def output_kaggle_test(preds):\n","  print(\"Hi\")\n","  kaggle_df = test_df[['id']].copy()\n","  kaggle_df['target'] = preds\n","  kaggle_df.to_csv('baseline.csv', index=False)\n","  display(kaggle_df)\n","\n","# preds = [disaster_baseline(x) for x in test_df['text']]\n","# # output_kaggle_test(preds)"],"metadata":{"id":"AYXr8pWzLxzF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The Kaggle score is: 0.60435"],"metadata":{"id":"uup-wSMDHAlq"}},{"cell_type":"markdown","source":["# **VISUALIZATION**"],"metadata":{"id":"l5SmZfHCKEpc"}},{"cell_type":"code","source":["# Created at a histogram of train set predictions.\n","plt.hist(train_df['target'])\n","plt.show()\n","# From this we can see that the majority of the class is non-disaster."],"metadata":{"id":"sP_sp9nrJozI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Created two lists for the length of text in each class\n","disaster_lengths = [len(text) for text in train_df[train_df['target'] == 1]['text']]\n","non_disaster_lengths = [len(text) for text in train_df[train_df['target'] == 0]['text']]\n","\n","# Plotted the histograms\n","plt.hist(disaster_lengths, bins=50, alpha=0.7, label='Disaster Tweets', density=True)\n","plt.hist(non_disaster_lengths, bins=50, alpha=0.5, label='Non-Disaster Tweets', density=True)\n","plt.xlabel('Length of Tweet')\n","plt.ylabel('Frequency')\n","plt.legend(loc='upper left')\n","plt.show()\n","\n","\n","# here we tried to see if we could predict the class based on the length of the tweet.\n","# We see that as the length of the tweet increases the tweet becomes more of a non disaster"],"metadata":{"id":"yOJmt7FglNHc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import collections\n","\n","# created a list of words from the test that are disaster\n","disaster_words = ' '.join(train_df[train_df['target']==1]['text']).split()\n","\n","# created a set of words from the disaster words\n","set_disaster_words = set(disaster_words)\n","\n","# created a list of words from the test that are not disaster\n","non_disaster_words = ' '.join(train_df[train_df['target']==0]['text']).split()\n","\n","# created a set of words from the non disaster words\n","set_non_disaster_words = set(non_disaster_words)\n","\n","# created a set of words that are only present in disaster words\n","only_disaster_word = set_disaster_words - set_non_disaster_words\n","\n","# created a dictionary of words with frequency\n","disaster_word_counts = collections.Counter(disaster_words)\n","\n","# created a dictionary of words thats only in disaster words with frequency\n","only_disaster_word_freq = {}\n","for word in only_disaster_word:\n","  only_disaster_word_freq[word] = disaster_word_counts[word]\n","# sorted the only_disaster_word_freq so that we can display the top 20 words in the bar chart\n","sorted_only_disaster_word_freq = sorted(only_disaster_word_freq.items(), key=lambda x: x[1], reverse=True)\n","print(sorted_only_disaster_word_freq)\n","\n","#the top 20 most frequent disaster words and their frequency.\n","top_words = sorted_only_disaster_word_freq[:20]\n","words = [word[0] for word in top_words]\n","counts = [word[1] for word in top_words]\n","\n","# Plotted a bar chart of the top 20 most frequent disaster words that are not present in the non-disaster tweets.\n","plt.bar(words, counts)\n","plt.title('Top 10 Most Frequent Disaster Words')\n","plt.xlabel('Words')\n","plt.ylabel('Frequency')\n","plt.xticks(rotation=90)\n","plt.show()\n","\n","# Below we can see the 20 most frequent words that are not present in the non-disaster tweets."],"metadata":{"id":"_fmcMvE-0jsd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we create a word_index that ordered by the freqquency they occur where each words map to the frequency in the dataset\n","# all words in the dataset.\n","all_words = ' '.join(train_df['text']).split()\n","print(len(all_words))\n","# frequency of each word.\n","word_counts = {}\n","for word in all_words:\n","    word = word.lower()\n","    if word in word_counts:\n","        word_counts[word] += 1\n","    else:\n","        word_counts[word] = 1\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(num_words=None, oov_token='<OOV>', lower=True)\n","tokenizer.fit_on_texts(all_words)\n","\n","word_index = tokenizer.word_index\n","print(word_index)\n","# Because we used index_from=3 (above), setting aside ids below 3 for special\n","# symbols, we need to add 3 to the index values.\n","word_counts = dict([(key, value+3) for (key, value) in word_counts.items()])\n","word_counts['<START>'] = 1  # start of input\n","word_counts['#'] = 2       # out-of-vocabulary (OOV)\n","word_counts['<UNUSED>'] = 3\n","# Sort words by frequency\n","print(word_counts)\n","\n","max_id = max(word_counts.values())\n","print('Largest ID:', max_id)"],"metadata":{"id":"1cI1JOYEWIKk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode(text):\n","  # Split the input text into individual words.\n","  words = text.lower().split()\n","\n","  # Look up the integer value of each word in the index dictionary.\n","  # If a word is not in the dictionary, use the integer value for the \"<OOV>\" token.\n","  token_ids = [word_index.get(word, 1) for word in words]\n","\n","  return token_ids\n","# Show the ids corresponding tokens in the first example.\n","encoded_tweets = [encode(x) for x in train_df['text']]\n","print(train_df['text'][1])\n","print(encode(train_df['text'][1]))\n","# print(encoded_tweets)"],"metadata":{"id":"0tIsQmlnpPAu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = train_df[['text', 'target']]\n","train_data['text'] = train_data['text'].apply(encode)\n","# print(train_data)\n","# split the data\n","training_data, validation_data = train_test_split(train_data, test_size=0.1, random_state=25)\n","print(validation_data)"],"metadata":{"id":"ajIa-zOq13qb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# As is clear from the length histogram, the current representation of the review text is a variable-length\n","# Since fixed-length arrays are easier to work with in Tensorflow, let's add special padding tokens at the end of\n","# each review until they are all the same length.\n","\n","# We'll also use this operation to limit the number of token positions by truncating all reviews to a specified length.\n","# In the code below, as an example, we pad all training inputs to length 300.\n","\n","def pad_data(sequences, max_length):\n","  # Keras has a convenient utility for padding a sequence.\n","  # Also make sure we get a numpy array rather than an array of lists.\n","  return np.array(list(\n","      tf.keras.preprocessing.sequence.pad_sequences(\n","          sequences, maxlen=max_length, padding='post', value=0)))\n","\n","# Pad and truncate to 300 tokens.\n","train_data_padded = pad_data(training_data['text'], max_length=300)\n","\n","# Check the padded output.\n","print('Length of X_train[0]:', len(training_data['text']))\n","print('Length of X_train_padded[0]:', len(train_data_padded[0]))\n","print(train_data_padded[0])"],"metadata":{"id":"5t6ennh8muvd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def limit_vocab(sequences, max_token_id, oov_id=1):\n","  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n","  reduced_sequences = np.copy(sequences)\n","  reduced_sequences[reduced_sequences >= max_token_id] = oov_id\n","  return reduced_sequences\n","# print(decode(X_train[0]))\n","# Reduce vocabulary to 1000 tokens.\n","X_train_reduced = limit_vocab(train_data_padded, max_token_id=1000)\n","\n","print(X_train_reduced[0])"],"metadata":{"id":"1hicxAem3Oz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Keras has a util to create one-hot encodings.\n","X_train_padded = pad_data(training_data['text'], max_length=20)\n","X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n","X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n","print(X_train_reduced[0])\n","print('X_train_one_hot shape:', X_train_one_hot.shape)"],"metadata":{"id":"b3s3M9v04SF3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_onehot_model(average_over_positions=False):\n","  \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n","  # Clear session and remove randomness.\n","  tf.keras.backend.clear_session()\n","  tf.keras.utils.set_random_seed(0)\n","\n","  model = tf.keras.Sequential()\n","  if average_over_positions:\n","    # This layer averages over the first dimension of the input by default.\n","    model.add(tf.keras.layers.GlobalAveragePooling1D())\n","  else:\n","    # Concatenate.\n","    model.add(tf.keras.layers.Flatten())\n","  # extra layer\n","  model.add(tf.keras.layers.Dense(\n","        units=50,                     # output dim (for binary classification)\n","        activation=\"sigmoid\"         # sigmoid activation for classification\n","    ))\n","\n","  model.add(tf.keras.layers.Dense(\n","      units=1,                     # output dim (for binary classification)\n","      activation=\"sigmoid\"         # sigmoid activation for classification\n","  ))\n","\n","  model.compile(loss='binary_crossentropy',   # this is a classification task\n","                optimizer='adam',             # fancy optimizer\n","                metrics=['accuracy'])\n","\n","  return model"],"metadata":{"id":"3PZI9Jo1-Hd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_history(history):\n","  plt.ylabel('Loss')\n","  plt.xlabel('Epoch')\n","  plt.xticks(range(0, len(history['loss'] + 1)))\n","  plt.plot(history['loss'], label=\"training\", marker='o')\n","  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n","  plt.legend()\n","  plt.show()"],"metadata":{"id":"pYlegz7V5AbO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = build_onehot_model()\n","\n","# Fit the model.\n","history = model.fit(\n","  x = X_train_one_hot,  # one-hot training data\n","  y = training_data['target'],          # corresponding binary labels\n","  epochs=5,             # number of passes through the training data\n","  batch_size=64,        # mini-batch size\n","  validation_split=0.1, # use a fraction of the examples for validation\n","  verbose=1             # display some progress output during training\n","  )\n","\n","# Convert the return value into a DataFrame so we can see the train loss\n","# and binary accuracy after every epoch.\n","history = pd.DataFrame(history.history)\n","plot_history(history)"],"metadata":{"id":"lH6cl-zt-Wo6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_embeddings_model(average_over_positions=False,\n","                           vocab_size=1000,\n","                           sequence_length=20,\n","                           embedding_dim=2):\n","  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n","  # Clear session and remove randomness.\n","  tf.keras.backend.clear_session()\n","  tf.keras.utils.set_random_seed(0)\n","\n","  model = tf.keras.Sequential()\n","  model.add(tf.keras.layers.Embedding(\n","      input_dim=vocab_size,\n","      output_dim=embedding_dim,\n","      input_length=sequence_length)\n","  )\n","\n","  if average_over_positions:\n","    # This layer averages over the first dimension of the input by default.\n","    model.add(tf.keras.layers.GlobalAveragePooling1D())\n","  else:\n","    # Concatenate.\n","    model.add(tf.keras.layers.Flatten())\n","\n","  # extra layer\n","  model.add(tf.keras.layers.Dense(\n","        units=50,                     # output dim (for binary classification)\n","        activation=\"sigmoid\"         # sigmoid activation for classification\n","    ))\n","\n","  model.add(tf.keras.layers.Dense(\n","      units=1,                     # output dim (for binary classification)\n","      activation='sigmoid'         # apply the sigmoid function!\n","  ))\n","\n","  model.compile(loss='binary_crossentropy',\n","                optimizer='adam',\n","                metrics=['accuracy'])\n","\n","  return model"],"metadata":{"id":"uZTbH1OT4-98"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = build_embeddings_model(average_over_positions=True,\n","                               vocab_size=1000,\n","                               sequence_length=20,\n","                               embedding_dim=64)\n","\n","history = model.fit(\n","  x = X_train_reduced,  # our sparse padded training data\n","  y = training_data['target'],          # corresponding binary labels\n","  epochs=5,             # number of passes through the training data\n","  batch_size=64,        # mini-batch size\n","  validation_split=0.1, # use a fraction of the examples for validation\n","  verbose=1             # display some progress output during training\n","  )\n","\n","history = pd.DataFrame(history.history)\n","plot_history(history)"],"metadata":{"id":"YofxqgWz5D_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Output CSV\n","print(test_df)\n","test_df['text'] = test_df['text'].apply(encode)\n","test_data_padded = pad_data(test_df['text'], max_length=20)\n","X_test_reduced = limit_vocab(test_data_padded, max_token_id=1000)\n","# print(X_test_reduced)\n","predicted_probabilities = model.predict(X_test_reduced)\n","print(predicted_probabilities)\n","predicted_classes = (predicted_probabilities > 0.5).astype(int)\n","output_kaggle_test(predicted_classes)"],"metadata":{"id":"EuGmWUOKJXY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_bow_model(vocab_size=1000,\n","                    sequence_length=20,\n","                    embedding_dim=2):\n","  model = tf.keras.Sequential()\n","  model.add(tf.keras.layers.Embedding(\n","      input_dim=vocab_size,\n","      output_dim=embedding_dim,\n","      input_length=sequence_length)\n","  )\n","  model.add(tf.keras.layers.GlobalAveragePooling1D())\n","  model.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n","  model.add(tf.keras.layers.Dense(units=32, activation='sigmoid'))\n","  model.add(tf.keras.layers.Dense(\n","      units=1,                     # output dim (for binary classification)\n","      activation='sigmoid'         # apply the sigmoid function!\n","  ))\n","\n","  model.compile(loss='binary_crossentropy', optimizer='adam',\n","                metrics=['accuracy'])\n","  return model\n","\n","bow_model = build_bow_model(\n","    vocab_size=1000, sequence_length=20, embedding_dim=16)\n","\n","X_train_padded = pad_data(training_data['text'], max_length=20)\n","X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n","X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n","\n","history = model.fit(\n","  x = X_train_reduced,  # our sparse padded training data\n","  y = training_data['target'],          # corresponding binary labels\n","  epochs=5,             # number of passes through the training data\n","  batch_size=64,        # mini-batch size\n","  validation_split=0.1, # use a fraction of the examples for validation\n","  verbose=1             # display some progress output during training\n","  )\n","\n","history = pd.DataFrame(history.history)\n","plot_history(history)"],"metadata":{"id":"RMytYPlkZ-dD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Functional Keras API\n","def build_cnn_model(vocab_size,\n","                    sequence_length,\n","                    embedding_dim):\n","  x = tf.keras.Input(shape=(sequence_length))\n","\n","  emb = tf.keras.layers.Embedding(input_dim=vocab_size,\n","                                  output_dim=embedding_dim,\n","                                  input_length=sequence_length)(x)\n","\n","  c1 = tf.keras.layers.Conv1D(\n","      filters=32, kernel_size=3, padding='same', activation='relu')(emb)\n","\n","  c2 = tf.keras.layers.Conv1D(\n","      filters=32, kernel_size=4, padding='same', activation='relu')(emb)\n","\n","  c3 = tf.keras.layers.Conv1D(\n","      filters=32, kernel_size=5, padding='same', activation='relu')(emb)\n","\n","\n","  y = tf.keras.layers.Concatenate()([c1, c2, c3])\n","  y = tf.keras.layers.Dropout(rate=0.05)(y)\n","  y = tf.keras.layers.MaxPool1D(pool_size=sequence_length)(y)\n","  y = tf.keras.layers.Flatten()(y)\n","  y = tf.keras.layers.Dense(units=1, activation='sigmoid')(y)\n","\n","  model = tf.keras.Model(inputs=x, outputs=y, name='imdb_cnn')\n","\n","  model.compile(loss='binary_crossentropy', optimizer='adam',\n","                metrics=['accuracy'])\n","  return model\n","\n","cnn_model = build_cnn_model(\n","    vocab_size=1000, sequence_length=20, embedding_dim=128)\n","\n","X_train_padded = pad_data(training_data['text'], max_length=20)\n","X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n","X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n","\n","history = model.fit(\n","  x = X_train_reduced,  # our sparse padded training data\n","  y = training_data['target'],          # corresponding binary labels\n","  epochs=5,             # number of passes through the training data\n","  batch_size=64,        # mini-batch size\n","  validation_split=0.1, # use a fraction of the examples for validation\n","  verbose=1             # display some progress output during training\n","  )\n","\n","history = pd.DataFrame(history.history)\n","plot_history(history)"],"metadata":{"id":"EOw-rK_ubvRa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_neural_network():\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n","        tf.keras.layers.MaxPooling2D(2,2),\n","        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","        tf.keras.layers.MaxPooling2D(2,2),\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.Dense(128, activation='relu'),\n","        tf.keras.layers.Dense(1, activation='softmax')\n","    ])\n","\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","neural_model = build_cnn_model(\n","    vocab_size=1000, sequence_length=20, embedding_dim=128)\n","\n","X_train_padded = pad_data(training_data['text'], max_length=20)\n","X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n","X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n","\n","history = neural_model.fit(\n","  x = X_train_reduced,  # our sparse padded training data\n","  y = training_data['target'],          # corresponding binary labels\n","  epochs=5,             # number of passes through the training data\n","  batch_size=64,        # mini-batch size\n","  validation_split=0.1, # use a fraction of the examples for validation\n","  verbose=1             # display some progress output during training\n","  )\n","\n","history = pd.DataFrame(history.history)\n","plot_history(history)"],"metadata":{"id":"lGe3fP5pmP-F"},"execution_count":null,"outputs":[]}]}