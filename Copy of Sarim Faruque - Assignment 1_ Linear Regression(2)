{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1eiICe-8b_EeuaDbyTJ5VvASoVNtM7Swj","timestamp":1675658815851},{"file_id":"1_Jc2W2gF4C_csBg_5ackxVADqXwt-EGN","timestamp":1674021886234},{"file_id":"1ird29mn_vuEDN558Lzmn_FEwlVB7yquS","timestamp":1673974198176},{"file_id":"1a_cjV0azE6XKPz9F7kqlV3jg9ro4otVd","timestamp":1626213873848},{"file_id":"1Xg093t365Cdu1_Y7GXuQdFrgJ017GpwB","timestamp":1578872277894},{"file_id":"https://github.com/google/eng-edu/blob/master/ml/recommendation-systems/recommendation-systems.ipynb","timestamp":1575485653875}],"collapsed_sections":["copyright"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"copyright"},"source":["#### Copyright 2021 Google LLC.\n","\n","SPDX-License-Identifier: Apache-2.0"]},{"cell_type":"code","metadata":{"id":"JxI782D_VLzp"},"source":["# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MKsRDH5ZUdfasdv"},"source":["# Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"43534tdfgs-v"},"source":["This assignment introduces linear regression with gradient descent, but starts with basic code building blocks.\n","\n","As you read through this notebook, you'll some mathematical notation that may not all be familiar. Don't panic! We will go through all of this in more detail in class and lab."]},{"cell_type":"markdown","metadata":{"id":"overview"},"source":["## Overview"]},{"cell_type":"code","source":[],"metadata":{"id":"xYbD2lTO4wsd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rY7emKinFRCr"},"source":["###Learning Objectives\n","* Practice manipulating numpy arrays.\n","* Generate artifical data, which can be useful for building intuition and debugging issues with complex models.\n","* Use Mean Squared Error to compare models.\n","* Understand generalization and overfitting.\n","* Work through the derivation of gradient descent for linear regression.\n","* Apply gradient descent to build intuition about training models."]},{"cell_type":"markdown","metadata":{"id":"6gKL1mrkyLux"},"source":["### Grading\n","\n","This assignment (exercises 1-8) is worth a total of 40 points."]},{"cell_type":"markdown","metadata":{"id":"rHLcriKWLRe4"},"source":["## Artificial Data"]},{"cell_type":"markdown","metadata":{"id":"kxDM3E2icDIg"},"source":["As a warm-up, let's get some practice manipulating matrices with numpy, plotting with matplotlib, and computing evaluation metrics for some simple models."]},{"cell_type":"code","metadata":{"id":"7X58hOMTUH-w"},"source":["# Import the libraries we'll use below.\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0nNOD-Z7SzAq"},"source":["## Data as matrices\n","Data usually comes in the form of matrices. The Python Numpy library makes it easy to manipulate matrices (*arrays* in numpy) efficiently. See the [Numpy Tutorial](https://docs.scipy.org/doc/numpy/user/quickstart.html) for details.\n","\n","A few important details to point out:\n","\n","* Numpy arrays follow the (row, column) indexing convention so the shape of a matrix with $r$ rows and $c$ columns is $(r,c)$.\n","* Remember that matrix multiplication requires the inner dimensions to align.\n","* A 1-dimensional array (*vector*) with $n$ elements has shape $(n,)$ and transposing it has no effect."]},{"cell_type":"code","metadata":{"id":"KWlmuAMwTZ3P"},"source":["# Print these to make sure you understand what is being generated.\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","A = np.array([1, 2, 3])\n","B = np.arange(1, 13).reshape(3, 4)\n","C = np.ones((2, 3))\n","D = np.eye(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4wvvzKoUIAN"},"source":["### Exercise 1: Matrix manipulation (7pts)\n","\n","Perform the following computations using numpy functions and print the results. Each can easily be expressed in a single line of code -- please use Numpy operations rather than for loops.\n","\n","\\begin{align}\n","1. && 2A + 1 \\\\\n","2. && \\textrm{Sum the rows of } B \\\\\n","3. && \\textrm{Sum the columns of } B \\\\\n","4. && \\textrm{Number of elements of $B$ greater than 5} \\\\\n","5. && C + C \\\\\n","6. && A * B \\\\\n","7. && (B * B^{\\textrm{transpose}}) - D \\\\\n","\\end{align}"]},{"cell_type":"markdown","metadata":{"id":"NiJInaRk6TbO"},"source":["#### Student Solution"]},{"cell_type":"code","metadata":{"id":"HJtwrjdO6TbS"},"source":["# YOUR CODE HERE \n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","A = np.array([1, 2, 3])\n","B = np.arange(1, 13).reshape(3, 4)\n","C = np.ones((2, 3))\n","D = np.eye(3)\n","print(A)\n","print('')\n","print(B)\n","print('')\n","print(C)\n","print('')\n","print(D)\n","print('')\n","print('')\n","#1\n","print(np.add(2*A,1))\n","print('')\n","#2\n","print(B[0].sum())\n","print(B[1].sum())\n","print(B[2].sum())\n","print('')\n","#3\n","print(B.sum(axis=0))\n","print('')\n","#4\n","print((B > 5).sum())\n","print('')\n","#5\n","print(np.add(C,C))\n","print('')\n","#6\n","print(np.dot(A,B))\n","print('')\n","#7\n","print(np.dot(B,B.transpose())-D)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uwkTfTVg6EuS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xbCRG2-uUKCT"},"source":["## Data for Supervised Learning\n","Supervised learning is all about learning to make predictions: given an input $x$ (e.g. home square footage), can we predict an output $\\hat{y}$ (e.g. estimated value) as close to the actual observed output $y$ (e.g. sale price) as possible. Note that the \"hat\" above $y$ is used to denote an estimated or predicted value. The *output* is sometimes referred to as a *target* or a *label*.\n","\n","Let's start by generating some artificial data. We'll create a vector of inputs, $X$, and a corresponding vector of target outputs $Y$. In general, we'll refer to invidual examples with a lowercase ($x$), and a vector or matrix containing multiple examples with a capital ($X$)."]},{"cell_type":"code","metadata":{"id":"Ulmn_bFdU87t"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n","  \"\"\"Create X, Y data with a linear relationship with added noise.\n","\n","  Args:\n","    num_examples: number of examples to generate\n","    w: desired slope\n","    b: desired intercept\n","    random_scale: add uniform noise between -random_scale and +random_scale\n","\n","  Returns:\n","    X and Y with shape (num_examples)\n","  \"\"\"\n","  X = np.arange(num_examples)\n","  np.random.seed(4)  # consistent random number generation\n","  noise = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n","  Y = w * X + b + noise\n","  return X, Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qJg0IiYVJ8U"},"source":["# Create some artificial data using create_1d_data.\n","X, Y = create_1d_data()\n","plt.scatter(X, Y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W6coKbXSpXOz"},"source":["### Exercise 2: Models for Data (3 pts)\n","A model is a function that takes an input $x$ and produces a prediction $\\hat{y}$.\n","\n","Let's consider two possible models for this data:\n","1. $M_1(x) = x+5 = \\hat{y^1}$\n","2. $M_2(x) = 2x+1 = \\hat{y^2}$\n","\n","Compute the predictions of models $M_1$ and $M_2$ for the values in $X$. These predictions should be vectors of the same shape as $Y$. Then plot the prediction lines of these two models overlayed on the \"observed\" data $(X, Y)$. Use [plt.plot()](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html) to draw the lines.\n","\n","Which of these models is better?"]},{"cell_type":"markdown","metadata":{"id":"V-z7qTVRJo5B"},"source":["#### Student Solution"]},{"cell_type":"code","metadata":{"id":"AHIY5kNXUIAP"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# YOUR CODE HERE\n","def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n","  \"\"\"Create X, Y data with a linear relationship with added noise.\n","\n","  Args:\n","    num_examples: number of examples to generate\n","    w: desired slope\n","    b: desired intercept\n","    random_scale: add uniform noise between -random_scale and +random_scale\n","\n","  Returns:\n","    X and Y with shape (num_examples)\n","  \"\"\"\n","  X = np.arange(num_examples)\n","  np.random.seed(4)  # consistent random number generation\n","  noise = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n","  Y = w * X + b + noise\n","  return X, Y\n","\n","# Create some artificial data using create_1d_data.\n","X_1, Y_1 = create_1d_data(10, 1, 5)\n","X_2, Y_2 = create_1d_data(10, 2, 1)\n","plt.scatter(X_1, Y_1)\n","plt.scatter(X_2, Y_2)\n","plt.show()\n","plt.plot(X_1, Y_1)\n","plt.plot(X_2, Y_2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NH-0soZiWx9x"},"source":["## Evaluation Metrics\n","\n","How good are our models? Intuitively, the better the model, the more closely it fits the data we have. That is, for each $x$, we'll compare $y$, the true value, with $\\hat{y}$, the predicted value. This comparison is often called the *loss* or the *error*. One common such comparison is *squared error*: $(y-\\hat{y})^2$. Averaging over all our data points, we get the *mean squared error*:\n","\n","\\begin{equation}\n","\\textit{MSE} = \\frac{1}{|Y|} \\sum_{y_i \\in Y}(y_i - \\hat{y_i})^2\n","\\end{equation}\n","\n","This is a very common way to evaluate models and heavily punishes predictions that are far away from the actual value. Linear Regression models are learned by optimizing $\\textit{MSE}$."]},{"cell_type":"markdown","metadata":{"id":"_AyY2DpxYLI0"},"source":["### Exercise 3: Computing MSE (3 pts)\n","Write a function for computing the MSE metric and use it to compute the MSE for the two models above, $M_1$ and $M_2$. You should be able to do this **without using a for loop**."]},{"cell_type":"markdown","metadata":{"id":"WmUXw-R9YykQ"},"source":["#### Student Solution"]},{"cell_type":"code","metadata":{"id":"uCeAfI5mW9sg"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# YOUR CODE HERE\n","def MSE(true_values, predicted_values):\n","  n = len(true_values)\n","  \"\"\"Return the MSE between true_values and predicted values.\"\"\"\n","  d = true_values - predicted_values\n","  s = d ** 2\n","  sumsquaredif = sum(s)\n","  return (1/n) * sumsquaredif\n","\n","pred = np.array([2,3,4])\n","\n","actual = np.array([1,1,4])\n","print(MSE(actual, pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eDiy3OZwZlwj"},"source":["## Generalization\n","\n","Our data $(X, Y)$ represents just a sample of all possible input-output pairs we might care about. A model will be useful to the extent we can apply it to new inputs. Consider the more complex model below, which appears to produce a much smaller mean squared error."]},{"cell_type":"code","metadata":{"id":"ns1siZ9DZvSY"},"source":["# Fit a 7-th degree polynomial to (X, Y). See np.polyfit for details.\n","polynomial_model_coefficients = np.polyfit(X, Y, deg=7)\n","polynomial_model = np.poly1d(polynomial_model_coefficients)\n","M3 = polynomial_model(X)\n","X_plot = np.linspace(0, 10, 100)\n","M3_plot = polynomial_model(X_plot)\n","fig = plt.scatter(X, Y)\n","plt.plot(X_plot, M3_plot, '-k')\n","print ('MSE for M3:', MSE(Y, M3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, we care about how well a model will apply to data we have not yet seen. Typically we do this by having a **training set**, which the models learns from, and a **test set**, which the model is evaluated on. You want to ensure that your model does not **overfit** to the training set, capturing infromation that does not generalize well.\n","\n","In general, ML practitioners try not to use `np.polyfit` models that have high degrees, as those do not generalize well. Especially for $X$ values that fall outside the range of the training data."],"metadata":{"id":"n3GIoYRjzn0j"}},{"cell_type":"markdown","metadata":{"id":"M2m9YmLMZ1EV"},"source":["### Exercise 4: Generalization (4 pts)\n","Apply models M2 and M3 to the test data below. Print the predicted outputs of the models and compare the MSE between the predictions and true values (Y_test)."]},{"cell_type":"code","metadata":{"id":"CUW1gSlHBxut"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","# Suppose we have the following test data:\n","X_test = np.array([10, 11, 12])\n","Y_test = np.array([22.1, 23.4, 26.2])\n","\n","# YOUR CODE HERE\n","def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n","  \"\"\"Create X, Y data with a linear relationship with added noise.\n","\n","  Args:\n","    num_examples: number of examples to generate\n","    w: desired slope\n","    b: desired intercept\n","    random_scale: add uniform noise between -random_scale and +random_scale\n","\n","  Returns:\n","    X and Y with shape (num_examples)\n","  \"\"\"\n","  X = np.arange(num_examples)\n","  np.random.seed(4)  # consistent random number generation\n","  noise = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n","  Y = w * X + b + noise\n","  return X, Y\n","\n","X, Y = create_1d_data()\n","\n","# Plot train and test data.\n","plt.scatter(X, Y, color='k', label='Train')\n","plt.scatter(X_test, Y_test, color='r', label='Test')\n","plt.legend()\n","plt.show()\n","\n","polynomial_model_coefficients = np.polyfit(X, Y, deg=7)\n","polynomial_model = np.poly1d(polynomial_model_coefficients)\n","M3 = polynomial_model(X)\n","X_plot = np.linspace(0, 10, 100)\n","M3_plot = polynomial_model(X_plot)\n","fig = plt.scatter(X, Y)\n","plt.plot(X_plot, M3_plot, '-k')\n","print ('MSE for M3:', MSE(Y, M3))\n","\n","print(MSE())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h0Zpx79_aQEC"},"source":["#### Student Solution"]},{"cell_type":"code","metadata":{"id":"HWsB_pQRDrk2"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","X_test = np.array([1,2,3,4,5,6,7,8,9,10])\n","Y_test = np.array([1,2,3,4,5,6,7,8,9,10])\n","\n","\n","def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n","  \"\"\"Create X, Y data with a linear relationship with added noise.\n","\n","  Args:\n","    num_examples: number of examples to generate\n","    w: desired slope\n","    b: desired intercept\n","    random_scale: add uniform noise between -random_scale and +random_scale\n","\n","  Returns:\n","    X and Y with shape (num_examples)\n","  \"\"\"\n","  X = np.arange(num_examples)\n","  np.random.seed(4)  # consistent random number generation\n","  noise = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n","  Y = w * X + b + noise\n","  return X, Y\n","\n","\n","\n","# YOUR CODE HERE\n","# Suppose we have the following test data:\n","\n","X_1, Y_1 = create_1d_data(10, 1, 5)\n","X_2, Y_2 = create_1d_data(10, 2, 1)\n","\n","print(Y_1)\n","print(Y_2)\n","print(Y_test)\n","plt.scatter(X_1, Y_1, color='k', label='Train')\n","plt.scatter(X_test, Y_test, color='r', label='Test')\n","plt.legend()\n","plt.show()\n","\n","plt.scatter(X_2, Y_2, color='k', label='Train')\n","plt.scatter(X_test, Y_test, color='r', label='Test')\n","plt.legend()\n","plt.show()\n","\n","def MSE(true_values, predicted_values):\n","  n = len(true_values)\n","  \"\"\"Return the MSE between true_values and predicted values.\"\"\"\n","  d = true_values - predicted_values\n","  s = d ** 2\n","  sumsquaredif = sum(s)\n","  return (1/n) * sumsquaredif\n","\n","print(MSE(Y_1,Y_test))\n","print(MSE(Y_2,Y_test))\n","\n","polynomial_model_coefficients = np.polyfit(X, Y, deg=7)\n","polynomial_model = np.poly1d(polynomial_model_coefficients)\n","M3 = polynomial_model(X)\n","X_plot = np.linspace(0, 10, 100)\n","M3_plot = polynomial_model(X_plot)\n","fig = plt.scatter(X, Y)\n","plt.plot(X_plot, M3_plot, '-k')\n","print ('MSE for M3:', MSE(Y, M3))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YCWuhbfLNmna"},"source":["## Notation\n","In our artificial data, things are pretty simple: each input example is just a single value. But soon, each input example will include multiple values or *features*, so we need some conventions to avoid confusion.\n","\n","Let's start with the inputs:\n","\n","\\begin{align}\n","X =\n","\\begin{pmatrix}\n","x_0 \\\\\n","x_1 \\\\\n","\\vdots \\\\\n","x_{m-1}\n","\\end{pmatrix}\n","\\end{align}\n","\n","* Capital $X$ refers to all input examples together.\n","* Lowercase $x$ refers to an individual input example; we use $x_i$ to refer to input example $i$; there are $m$ total examples.\n","\n","Further, each input example $x$ could itself be a vector of feature values:\n","\n","\\begin{align}\n","x_i = [x_{i,0}, x_{i,1}, \\dots x_{i,n-1}]\n","\\end{align}\n","\n","* Lowercase $x$ refers to all input features together for an individual input example.\n","* $x_i$ refers to feature $i$ for an input example $x$; there are $n$ total features.\n","\n","For example, $x_i$ could be an individual person. Then, $x_{i,j}$ is some attribute about the person like height. $X$ is the matrix of all people,.\n","\n","Similarly, we can index labels $y_i$ in $Y$, which we can think of as a column vector where $y_i$ is the label for $x_i$.\n","\n","\\begin{align}\n","Y =\n","\\begin{pmatrix}\n","y_0 \\\\\n","y_1 \\\\\n","\\vdots \\\\\n","y_{m-1}\n","\\end{pmatrix}\n","\\end{align}\n","\n","From now on, we will be using matrix notation by default. Rows refer to examples and columns refer to features. If we want to be very specific and refer to a particular feature of a particular input example, we can use $x_{i,j}$ for input $i$, feature $j$. Using matrices will be useful for coding ML algorithms since most of the operations we will do can be expressed as operations on matrices.\n"]},{"cell_type":"markdown","metadata":{"id":"2szkkNDvsCfn"},"source":["##Parameter Vectors\n","Let's prepare to learn a linear model $M(x)$ that approximates values of $Y$ from corresponding values of $X$. Since our input data has only one feature, our model will have two parameters (also called weights), which we'll refer to collectively as $W$:\n","\n","\\begin{align}\n","M(x) = w_0 + w_1x\n","\\end{align}\n","\n","If we use the model from our data generation (`create_1d_data`), $w_1=2$ and $w_0=1$.\n","\n","We can make this more concise. Notice that, if we prepend an extra feature (column) to $X$ that is always $1$, we can rewrite our model using a matrix multiplication:\n","\n","\\begin{align}\n","M(x) = w_0x_0 + w_1x_1 = xW^T\n","\\end{align}\n","\n","To make this matrix formulation as clear as possible, this is:\n","\n","\\begin{align}\n","\\hat{y} = xW^T =\n","\\begin{pmatrix}\n","x_0 & x_1 \\\\\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","w_0 \\\\\n","w_1 \\\\\n","\\end{pmatrix}\n","\\end{align}\n","\n","In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n","\n","\\begin{align}\n","\\hat{Y} = XW^T =\n","\\begin{pmatrix}\n","x_{0,0} & x_{0,1} \\\\\n","x_{1,0} & x_{1,1} \\\\\n","\\vdots & \\vdots \\\\\n","x_{m-1,0} & x_{m-1,1} \\\\\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","w_0 \\\\\n","w_1 \\\\\n","\\end{pmatrix}\n","\\end{align}\n","\n","Note that this notation does not care about number of features - if we were to add more features to $X$ and more weights to $W$, this notation would still hold up.\n","\n","Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up:\n","\n","\\begin{align}\n","X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n","\\end{align}"]},{"cell_type":"markdown","metadata":{"id":"8NXo1n9j1LMT"},"source":["### Exercise 5: Practice with Parameters (5 pts)\n","\n","Rewrite $M_1$ and $M_2$ (from exercise 2) using the above notation.\n","\n","This will required adding a column of $1$s to $X$ and using matrix multiplication (`np.dot`).\n","\n","Assert that your new values match the previous predictions, and that the shape of your predictions also match the shape of your labels $Y$."]},{"cell_type":"markdown","metadata":{"id":"R0FIiQ-j6tgn"},"source":["#### Student Solution"]},{"cell_type":"code","metadata":{"id":"aBEZ_QOX6qOi"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Add a column of 1s to X by using np.c_ to concatenate with the current values.\n","X = np.array([1,2,3,4,5,6,7,8,9])\n","X_with_1s = np.c_[np.ones(X.shape[0]), X]\n","print(X_with_1s)\n","#M1(x)=x+5=y1 \n","#M2(x)=2x+1=y2\n","m1 = np.array([1,5])\n","m2 = np.array([2,1])\n","print(np.dot(X_with_1s,m1))\n","print(np.dot(X_with_1s,m2))\n","\n","\n","\n","# YOUR CODE HERE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUNVK2acFMQ0"},"source":["## Gradient Descent\n","Here, we'll demonstrate gradient descent for linear regression to learn the weight vector $W$. We'll use the more specific notation $M_W(x)$ since we want to specify that $M$ is parameterized by $W$. As above, we'll assume that $x_0=1$ so we can write $M$ as a sum or a matrix product:\n","\n","\\begin{align}\n","M_W(x) = \\hat{Y} = \\sum_{i=0}^{n-1} w_i x_i = x W^T\n","\\end{align}\n","\n","To make the math more readable the derivation that follows, we'll use summations. However, when we implement this in the code below, we'll use matrix computations.\n","\n","In linear regression, we compute the loss, $J(W)$ from the mean squared difference between predictions $M_W(x)$ and targets $y$. In the following equation, we average the loss over each of the $m$ training examples.\n","\n","\\begin{align}\n","J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (M_W(x^{(i)}) - y^{(i)})^2\n","\\end{align}\n","\n","You'll notice that, unlike in our $\\textit{MSE}$ calculation above, we divide by $2$ here. Dividing by $2$ simplifies the formula of the gradient, since it cancels out the constant $2$ from by the derivative of the squared term (see below). Because error is relative to other models' errors, it does not matter the we divide by $2$ as long as we do it to every error.\n","\n","Remember that the gradient is a vector of partial derivatives for each $w_j$ (holding the other elements of $w$ constant). The gradient points in direction of steepest ascent for the loss function $J$.\n","\n","Here we derive the parameter update rule by computing the gradient of the loss function. We need a derivative for each feature in $x$, so we'll show how to compute the derivative with respect to $w_j$. For simplicity, let's assume we have only one training example ($m = 1$):\n","\n","\\begin{align}\n","\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{\\partial}{\\partial w_j} \\frac{1}{2} (h_W(x) - y)^2 \\tag{1}\\\\\n","&= 2 \\cdot \\frac{1}{2} (h_W(x) - y) \\cdot \\frac{\\partial}{\\partial w_j} (h_W(x) - y)\\tag{2} \\\\\n","&= (h_W(x) - y) \\frac{\\partial}{\\partial w_j} \\left(\\sum_{i=0}^{n-1} w_i x_i - y_i \\right) \\\\\n","&= (h_W(x) - y)x_j\n","\\end{align}\n","\n","The derivation has 2 key steps:\n","\n","(1) Apply the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) (step 1 -> 2).\n","\n","(2) The derivative with respect to $w_j$ of $h_W(x) - y$ is only non-zero for $w_j x_j$; $y_i$ does not depend on $w_j$, nor do any values $w_i x_i$ where $i \\neq j$. For this component, the derivative is $x_j$ (step 2 -> 4).\n","\n","Ok, that's it. We can now implement gradient descent for linear regression. The only difference in the code below is that it computes the loss as an average over all training examples (rather than just a single example)."]},{"cell_type":"markdown","metadata":{"id":"QaXYiTm9ftRf"},"source":["### Exercise 6: Implementing Gradient Descent for Linear Regression (8 pts)\n","Fill in the `NotImplemented` parts of the gradient descent function below. There are detailed comments to help guide you. Note that this function uses vectors and matrices so you'll want to use numpy functions like `np.dot` instead of loops."]},{"cell_type":"markdown","metadata":{"id":"gTlTUJkS4DzQ"},"source":["##### Student Solution"]},{"cell_type":"code","metadata":{"id":"_hP9rzDyFXTg"},"source":["def gradient_descent(inputs, outputs, learning_rate, num_epochs):\n","  \"\"\"Apply the gradient descent algorithm to learn learn linear regression.\n","\n","  Args:\n","    inputs: A 2-D array where each column is an input feature and each\n","            row is a training example.\n","    outputs: A 1-D array containing the real-valued\n","             label corresponding to the input data in the same row.\n","    learning_rate: The learning rate to use for updates.\n","    num_epochs: The number of passes through the full training data.\n","\n","  Returns:\n","    weights: A 2-D array with the learned weights after each training epoch.\n","    losses: A 1-D array with the loss after each epoch.\n","  \"\"\"\n","  # m = number of examples, n = number of features\n","  m, n = inputs.shape\n","\n","  # We'll use a vector of size n to store the learned weights and initialize\n","  # all weights to 1.\n","  W = np.ones(n)\n","\n","  # Keep track of the training loss and weights after each step.\n","  losses = []\n","  weights = []\n","\n","  for epoch in range(num_epochs):\n","      # Append the old weights to the weights list to keep track of them.\n","      weights.append(W)\n","\n","      # Evaluate the current predictions for the training examples given\n","      # the current estimate of W (you did this in exercise 5).\n","      predictions = NotImplemented\n","\n","      # Find the difference between the predictions and the actual targetvalues.\n","      diff = NotImplemented\n","\n","      # In standard linear regression, we want to minimize the sum of squared\n","      # differences. Find the loss as we did in Excerise 3, but use the extra\n","      # scaling factor (as in Formula (1) above, and discussed in class).\n","      loss = NotImplemented\n","\n","      # Append the loss to the losses list to keep a track of it.\n","      losses.append(loss)\n","\n","      # Compute the gradient with respect to the loss.\n","      # [Formula (4) in the Gradient Descent Implementation]\n","      gradient = NotImplemented\n","\n","      # Update weights, scaling the gradient by the learning rate.\n","      W = W - learning_rate * gradient\n","\n","  return np.array(weights), np.array(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kXN_YY-daSPK"},"source":["Let's try running gradient descent with our artificial data and print out the results. Note that we're passing the version of the input data with a column of $1s$ so that we learn an *intercept* (also called a *bias*). We can also try learning without the intercept.\n","\n","Note: if your implementation of gradient descent is correct, you should get a loss of ~0.20458 after 5 epochs (with a bias parameter)."]},{"cell_type":"code","metadata":{"id":"B4z23bKHayGU"},"source":["print('Running gradient descent...')\n","weights, losses = gradient_descent(X_with_1s, Y, learning_rate=.02,\n","                                   num_epochs=5)\n","for W, loss in zip(weights, losses):\n","  print(loss, W)\n","\n","print('\\nRunning gradient descent without biases...')\n","# Make sure we're providing an input with the right 2-D shape.\n","X_without_1s = np.expand_dims(X, axis=0).T\n","weights_without_bias, losses_without_bias = gradient_descent(X_without_1s, Y,\n","                                                             .02, num_epochs=5)\n","for W, loss in zip(weights_without_bias, losses_without_bias):\n","  print(loss, W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7vGx68Ec2Si"},"source":["### Exercise 7: Interpreting the Model (5 pts)\n","Write down the learned model with and without an intercept term. Which model fits the data better? How are you sure?"]},{"cell_type":"markdown","metadata":{"id":"ZlVWFCRtqZIZ"},"source":["#### Student Solution\n","\n","WRITE YOUR ANSWERS HERE"]},{"cell_type":"markdown","metadata":{"id":"g-DdmWmPrDvk"},"source":["## Gradient Descent Progress\n","Let's write a function that lets us visualize the progress of gradient descent during training. Our gradient descent function already provides intermediate weight vectors and losses after each epoch, so we just need to plot these."]},{"cell_type":"code","metadata":{"id":"lWcTeR_gNz9d"},"source":["def plot_learning(inputs, outputs, weights, losses):\n","  \"\"\"Plot predictions and losses after each training epoch.\n","\n","  Args:\n","    inputs: A 2-D array where each column is an input feature and each\n","            row is a training example.\n","    outputs: A 1-D array containing the real-valued\n","             label corresponding to the input data in the same row.\n","    weights: A 2-D array with the learned weights after each training epoch.\n","    losses: A 1-D array with the loss after each epoch.\n","  \"\"\"\n","  # Create a figure.\n","  plt.figure(1, figsize=[10,4])\n","\n","  # The first subplot will contain the predictions. Start by plotting the\n","  # outputs (Y).\n","  plt.subplot(121)\n","  plt.xlabel('x')\n","  plt.ylabel('y')\n","  plt.xticks(inputs[:,1])\n","  plt.scatter(inputs[:,1], outputs, color='black', label='Y')\n","\n","  # For each epoch, retrieve the estimated weights W, compute predictions, and\n","  # plot the resulting line.\n","  num_epochs = len(weights)\n","  for i in range(num_epochs):\n","    W = weights[i]\n","    predictions = np.dot(inputs, W.T)\n","    plt.plot(inputs[:,1], predictions, label='Epoch %d' %i)\n","  plt.legend()\n","\n","  # The second subplot will contain the losses.\n","  plt.subplot(122)\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Loss')\n","  plt.xticks(range(num_epochs))\n","  plt.plot(range(num_epochs), losses, marker='o', color='black',\n","           linestyle='dashed')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WECpzvgkrkFk"},"source":["### Exercise 8: Plotting Progress (5 pts)\n","\n","Re-run gradient descent using X_with_1s, but this time with learning_rate=0.01 and num_epochs=7.\n","\n","Run the plot_learning function using the weights and losses returned by gradient_descent (from above) and answer the following questions:\n","\n","1. Is learning converging faster or slower with `learning rate=0.01` than when we used `learning_rate=0.02`?\n","2. If you continue training, will the loss eventually reach 0? Why?\n","3. If you continue training, will the model eventually converge to $h(x)=2x+1$? Why?"]},{"cell_type":"markdown","metadata":{"id":"OjfwzNhzrB1D"},"source":["#### Student Solution"]},{"cell_type":"markdown","metadata":{"id":"G-mgY_e5upTL"},"source":["WRITE YOUR ANSWERS HERE\n","\n","1.\n","2.\n","3."]}]}